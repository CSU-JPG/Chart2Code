# Description:
# This script reads a JSON file to get image paths, generates Python code to reproduce the plots in those images
# using the InternVL-3-38B-Instruct model, and saves the results. It incorporates advanced dynamic image
# preprocessing to handle high-resolution inputs effectively and can be configured via command-line arguments.

import os
import torch
import re
import shutil
import csv
import math
import torch
from tqdm import tqdm
from PIL import Image
from transformers import AutoModel, AutoTokenizer, AutoConfig
import torchvision.transforms as T
from torchvision.transforms.functional import InterpolationMode
import argparse
import json

# ===================================================================================
# Model Configuration (Modify here)
# ===================================================================================
# 1. Model ID on Hugging Face Hub (for online loading)
HUB_MODEL_ID = "OpenGVLab/InternVL3-38B-Instruct" # Note: Official name might differ, adjust if needed

# 2. Path to the local model (relative to this .py file)
#    This script is assumed to be in a directory like .../Inference/level1_direct/models/
#    This path goes up to find the .../Inference/models/ folder
script_dir = os.path.dirname(os.path.abspath(__file__))
# Example: Go back from .../Inference/level1_direct/models/ to .../Inference/ and then into models/
LOCAL_MODEL_PATH = os.path.abspath(os.path.join(script_dir, '..', '..', 'models', 'InternVL3-38B-Instruct'))
# ===================================================================================


# --- Global Variables ---
# tokenizer and model will be loaded in the main function
tokenizer = None
model = None

# --- New Prompt Template (Do not change) ---
PROMPT_TEMPLATE = """You are a Python developer proficient in data visualization, with expertise in using libraries such as Matplotlib, NetworkX, Seaborn, and others.I have a plot generated by Python code, but I don't have the corresponding code that generated this plot. Your task is to generate the Python code that can perfectly reproduce the picture based on the image I provide.

Here are the requirements for the task:
1. **Data Extraction**: Extract the actual data from the provided image. Based on the visual features of the plot, you must infer the data and recreate the plot.
2. **Recreate the Image**: Generate the Matplotlib code that reproduces the image exactly as it appears, including all elements such as:
   - Plot type (scatter, line, bar, etc.)
   - Axis labels and titles
   - Colors, markers, line styles, and other visual styles
   - Any legends, annotations, or gridlines present in the image
3. **Self-contained Code**: The Python code should be complete, executable, and self-contained. It should not require any external data files or variables not already present in the code.
Your objective is to extract the any necessary details from the image and generate a Python script that accurately reproduces the plot.

Now, please generate the Python code to reproduce the picture below.
The output format must be strictly as follows:

```python
# Your Python code here to reproduce the image.
```"""

# --- Inference Parameters (Do not change) ---
INFERENCE_PARAMS = {
    "temperature": 0.1,
    "top_p": 0.9,
    "max_new_tokens": 4096, # Was 8192, adjusted for consistency
    "do_sample": True
}


# --- Model-Specific Helper Functions for InternVL (Do not change) ---
def split_model(model_path):
    device_map = {}
    world_size = torch.cuda.device_count()
    config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)
    num_layers = config.llm_config.num_hidden_layers
    # Since the first GPU will be used for ViT, treat it as half a GPU.
    num_layers_per_gpu = math.ceil(num_layers / (world_size - 0.5))
    num_layers_per_gpu = [num_layers_per_gpu] * world_size
    num_layers_per_gpu[0] = math.ceil(num_layers_per_gpu[0] * 0.5)
    layer_cnt = 0
    for i, num_layer in enumerate(num_layers_per_gpu):
        for j in range(num_layer):
            device_map[f'language_model.model.layers.{layer_cnt}'] = i
            layer_cnt += 1
    device_map['vision_model'] = 0
    device_map['mlp1'] = 0
    device_map['language_model.model.tok_embeddings'] = 0
    device_map['language_model.model.embed_tokens'] = 0
    device_map['language_model.output'] = 0
    device_map['language_model.model.norm'] = 0
    device_map['language_model.model.rotary_emb'] = 0
    device_map['language_model.lm_head'] = 0
    device_map[f'language_model.model.layers.{num_layers - 1}'] = 0
    return device_map

IMAGENET_MEAN = (0.485, 0.456, 0.406)
IMAGENET_STD = (0.229, 0.224, 0.225)

def build_transform(input_size):
    MEAN, STD = IMAGENET_MEAN, IMAGENET_STD
    transform = T.Compose([
        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),
        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),
        T.ToTensor(),
        T.Normalize(mean=MEAN, std=STD)
    ])
    return transform

def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):
    best_ratio_diff = float('inf')
    best_ratio = (1, 1)
    area = width * height
    for ratio in target_ratios:
        target_aspect_ratio = ratio[0] / ratio[1]
        ratio_diff = abs(aspect_ratio - target_aspect_ratio)
        if ratio_diff < best_ratio_diff:
            best_ratio_diff = ratio_diff
            best_ratio = ratio
        elif ratio_diff == best_ratio_diff:
            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:
                best_ratio = ratio
    return best_ratio

def dynamic_preprocess(image, min_num=1, max_num=12, image_size=448, use_thumbnail=False):
    orig_width, orig_height = image.size
    aspect_ratio = orig_width / orig_height
    target_ratios = set(
        (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if
        i * j <= max_num and i * j >= min_num)
    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])
    target_aspect_ratio = find_closest_aspect_ratio(
        aspect_ratio, target_ratios, orig_width, orig_height, image_size)
    target_width = image_size * target_aspect_ratio[0]
    target_height = image_size * target_aspect_ratio[1]
    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]
    resized_img = image.resize((target_width, target_height))
    processed_images = []
    for i in range(blocks):
        box = (
            (i % (target_width // image_size)) * image_size,
            (i // (target_width // image_size)) * image_size,
            ((i % (target_width // image_size)) + 1) * image_size,
            ((i // (target_width // image_size)) + 1) * image_size
        )
        split_img = resized_img.crop(box)
        processed_images.append(split_img)
    assert len(processed_images) == blocks
    if use_thumbnail and len(processed_images) != 1:
        thumbnail_img = image.resize((image_size, image_size))
        processed_images.append(thumbnail_img)
    return processed_images

def process_image(image, input_size=448, max_num=12):
    transform = build_transform(input_size=input_size)
    images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)
    pixel_values = [transform(img) for img in images]
    pixel_values = torch.stack(pixel_values)
    return pixel_values


# --- Code Extraction Function (Do not change) ---
def extract_python_code(raw_text: str) -> str:
    """
    Extracts Python code from a markdown-formatted string.
    Returns an empty string if no code block is found.
    """
    match = re.search(r"```python\s*\n(.*?)\n```", raw_text, re.DOTALL)
    if match:
        return match.group(1).strip()
    
    match = re.search(r"```\s*\n(.*?)\n```", raw_text, re.DOTALL)
    if match:
        return match.group(1).strip()
        
    return ""

# --- Core Inference Function (Do not change) ---
def generate_code_for_image(image_path: str) -> str:
    """Generates Python code for a given image using the InternVL model."""
    try:
        image = Image.open(image_path).convert('RGB')
        pixel_values = process_image(image).to(torch.bfloat16).cuda()
        
        with torch.no_grad():
            response = model.chat(tokenizer, pixel_values, PROMPT_TEMPLATE, INFERENCE_PARAMS)
        
        return response.strip()

    except Exception as e:
        tqdm.write(f"   - Error: Exception caught during inference -> {os.path.basename(image_path)} | Error: {e}")
        return None

# --- Batch Processing Main Flow ---
def main():
    # --- Added: More flexible command-line argument parsing ---
    parser = argparse.ArgumentParser(description="Generate Python code for images specified in a JSON file using the InternVL-3-38B model.")
    parser.add_argument('--load_source', type=str, choices=['local', 'hub'], default='hub', 
                        help="Select model loading source: 'local' (from a local path) or 'hub' (download from Hugging Face Hub).")
    parser.add_argument('--json_path', type=str, required=True, help="Path to the input JSON file containing image paths.")
    parser.add_argument('--output_dir', type=str, required=True, help="Directory to save the generated Python code.")
    parser.add_argument('--log_path', type=str, required=True, help="Path to save the CSV log file.")
    args = parser.parse_args()

    # --- Determine which model to load based on the --load_source argument ---
    if args.load_source == 'hub':
        model_to_load = HUB_MODEL_ID
        print("Mode: Loading model from Hugging Face Hub.")
    else: # 'local'
        model_to_load = LOCAL_MODEL_PATH
        print("Mode: Loading model from local path.")
        if not os.path.isdir(model_to_load):
            print(f"❌ Error: Local model path not found: '{model_to_load}'")
            print("Please check if the LOCAL_MODEL_PATH variable at the top of the file is configured correctly, or confirm that the model files exist.")
            exit(1)

    # --- Model Loading ---
    global tokenizer, model
    print(f"Loading InternVL-3-38B-Instruct model and tokenizer from '{model_to_load}'...")
    try:
        # InternVL-specific multi-GPU mapping
        device_map = split_model(model_to_load)
        
        model = AutoModel.from_pretrained(
            model_to_load,
            torch_dtype=torch.bfloat16,
            low_cpu_mem_usage=True,
            use_flash_attn=True,
            trust_remote_code=True,
            device_map=device_map).eval()
        tokenizer = AutoTokenizer.from_pretrained(model_to_load, trust_remote_code=True, use_fast=False)
        print("✅ InternVL model and tokenizer loaded successfully!")
    except Exception as e:
        print(f"❌ Model loading failed: {e}")
        exit()

    # --- Configure paths from command-line arguments ---
    JSON_FILE_PATH = args.json_path
    GENERATED_CODE_DIR = args.output_dir
    LOG_CSV_PATH = args.log_path
    FAILED_FILES_DIR = f"{GENERATED_CODE_DIR}_failed"

    os.makedirs(GENERATED_CODE_DIR, exist_ok=True)
    os.makedirs(FAILED_FILES_DIR, exist_ok=True)

    # --- Read image list from JSON file ---
    try:
        with open(JSON_FILE_PATH, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        script_dir = os.path.dirname(os.path.abspath(__file__))
        data_dir = os.path.abspath(os.path.join(script_dir, '..', '..', '..', 'data'))
        
        image_files = []
        for item in data:
            if "input image" in item and item["input image"]:
                relative_path = item["input image"]
                full_path = os.path.join(data_dir, relative_path)
                image_files.append(full_path)
    except FileNotFoundError:
        print(f"❌ Error: JSON file not found -> '{JSON_FILE_PATH}'")
        return
    except json.JSONDecodeError:
        print(f"❌ Error: Could not parse JSON file -> '{JSON_FILE_PATH}'")
        return
    except Exception as e:
        print(f"❌ Error: An unknown error occurred while reading the JSON file: {e}")
        return

    if not image_files:
        print(f"❌ Error: No valid 'input image' entries found in the JSON file '{JSON_FILE_PATH}'.")
        return

    total_files = len(image_files)
    print(f"Found {total_files} images in '{os.path.basename(JSON_FILE_PATH)}', starting processing...")

    success_count = 0
    failure_count = 0

    try:
        with open(LOG_CSV_PATH, 'w', newline='', encoding='utf-8') as csvfile:
            csv_writer = csv.writer(csvfile)
            csv_writer.writerow(["Figure Filename", "Status", "Instruction", "Raw Model Output", "Extracted Code"])

            for image_path in tqdm(image_files, desc="Processing progress"):
                if not os.path.exists(image_path):
                    tqdm.write(f"   - Warning: Image file not found, skipping -> {image_path}")
                    csv_writer.writerow([os.path.basename(image_path), "SKIPPED_NOT_FOUND", "N/A", "N/A", "N/A"])
                    continue
                
                base_name, _ = os.path.splitext(os.path.basename(image_path))
                output_path = os.path.join(GENERATED_CODE_DIR, f"{base_name}.py")
                figure_filename = os.path.basename(image_path)

                if os.path.exists(output_path):
                    tqdm.write(f"   - Skipping: Output file '{output_path}' already exists")
                    csv_writer.writerow([figure_filename, "SKIPPED_EXISTS", PROMPT_TEMPLATE, "N/A", "N/A"])
                    continue

                raw_generated_text = generate_code_for_image(image_path)
                
                if raw_generated_text:
                    clean_code = extract_python_code(raw_generated_text)
                    
                    if clean_code:
                        with open(output_path, 'w', encoding='utf-8') as f:
                            f.write(clean_code)
                        csv_writer.writerow([figure_filename, "SUCCESS", PROMPT_TEMPLATE, raw_generated_text, clean_code])
                        success_count += 1
                    else:
                        failure_count += 1
                        tqdm.write(f"   - Failure: Failed to extract valid code from model output -> {base_name}")
                        csv_writer.writerow([figure_filename, "FAILURE_EXTRACT", PROMPT_TEMPLATE, raw_generated_text, ""])
                        shutil.copy(image_path, FAILED_FILES_DIR)
                else:
                    failure_count += 1
                    tqdm.write(f"   - Failure: Error occurred during inference -> {base_name}")
                    csv_writer.writerow([figure_filename, "FAILURE_INFERENCE", PROMPT_TEMPLATE, "N/A", "N/A"])
                    try:
                        shutil.copy(image_path, FAILED_FILES_DIR)
                    except Exception as e:
                        tqdm.write(f"   - Critical Error: An error also occurred while copying the failed file: {e}")

    except IOError as e:
        print(f"❌ Critical Error: Could not write to CSV log file '{LOG_CSV_PATH}'. Error: {e}")
    finally:
        print(f"\n--- ✅ Batch processing complete (Model: InternVL-3-38B-Instruct) ---")
        print(f"Detailed log saved to: {LOG_CSV_PATH}")
        print(f"Successfully processed: {success_count} | Failed: {failure_count}")

if __name__ == "__main__":
    main()



